# DoD Budget CI Workflow
#
# Runs on every push and pull request to main.
# Matrix: Python 3.11 and 3.12.
#
# Test groups (separated for easy failure diagnosis):
#   1. Lint + type check
#   2. Unit tests — utilities, patterns, config, validation, query builders
#   3. API tests — endpoints, models, search, download, rate limiting
#   4. Frontend tests — routes, helpers, GUI features, fixes, accessibility
#   5. Data pipeline tests — build, enrichment, schema, pipeline
#   6. Performance tests — load, benchmarks, optimization
#   7. Docker build validation

name: CI

on:
  push:
    branches: [main, "claude/**"]
  pull_request:
    branches: [main]

jobs:
  test:
    name: "Test (Python ${{ matrix.python-version }})"
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.11", "3.12"]

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install ruff pytest pytest-cov mypy pyyaml fpdf2

      - name: Lint with ruff
        run: |
          ruff check . --select=E,W,F --ignore=E501 --exclude=DoD_Budget_Documents
        continue-on-error: true

      - name: Type check with mypy
        run: |
          mypy api/ utils/ --ignore-missing-imports --no-error-summary
        continue-on-error: true

      # ── Unit tests: utilities, patterns, config, validation ──────────────
      - name: "Unit tests: utilities & patterns"
        run: |
          python -m pytest \
            tests/test_common_utils.py \
            tests/test_strings_edge_cases.py \
            tests/test_patterns.py \
            tests/test_formatting.py \
            tests/test_cache_utils.py \
            tests/test_database_utils.py \
            -v --tb=short

      - name: "Unit tests: config & validation"
        run: |
          python -m pytest \
            tests/test_config_classes.py \
            tests/test_validation.py \
            tests/test_validation_classes.py \
            tests/test_validate_budget_db.py \
            -v --tb=short

      - name: "Unit tests: query builders"
        run: |
          python -m pytest \
            tests/test_query_builders.py \
            -v --tb=short

      # ── API tests: endpoints, models, search, download ──────────────────
      - name: "API tests: endpoints & models"
        run: |
          python -m pytest \
            tests/test_api.py \
            tests/test_api_models.py \
            tests/test_api_database.py \
            tests/test_api_search_snippet.py \
            tests/test_app_factory.py \
            tests/test_budget_lines_endpoint.py \
            tests/test_search_endpoint.py \
            tests/test_rate_limiter.py \
            -v --tb=short

      - name: "API tests: download & charts"
        run: |
          python -m pytest \
            tests/test_download.py \
            tests/test_charts_data.py \
            tests/test_reference_aggregation.py \
            -v --tb=short

      # ── Frontend tests: routes, helpers, GUI, accessibility ─────────────
      - name: "Frontend tests: routes & helpers"
        run: |
          python -m pytest \
            tests/test_frontend_routes.py \
            tests/test_frontend_helpers.py \
            tests/test_gui_features.py \
            tests/test_gui_fixes.py \
            tests/test_accessibility.py \
            -v --tb=short

      # ── Data pipeline tests ─────────────────────────────────────────────
      - name: "Pipeline tests: build, enrichment, schema"
        run: |
          python -m pytest \
            tests/test_build_integration.py \
            tests/test_schema_design.py \
            tests/test_enrich_budget_db.py \
            tests/test_checkpoint.py \
            tests/test_exhibit_inventory.py \
            tests/test_exhibit_audit.py \
            tests/test_exhibit_catalog.py \
            tests/test_parsing.py \
            tests/test_manifest.py \
            tests/test_backfill.py \
            -v --tb=short

      - name: "Pipeline tests: full pipeline integration"
        continue-on-error: true
        run: |
          python -m pytest tests/test_pipeline.py -v --tb=short

      # ── Performance & optimization tests ────────────────────────────────
      - name: "Performance & optimization tests"
        run: |
          python -m pytest \
            tests/test_performance.py \
            tests/test_optimizations.py \
            -v --tb=short

      # ── Code quality & pre-commit checks ────────────────────────────────
      - name: "Pre-commit quality checks"
        run: |
          python -m pytest \
            tests/test_precommit_checks.py \
            tests/test_precommit_hook.py \
            -v --tb=short

      # ── Remaining tests (BEAR, TIGER, etc.) ─────────────────────────────
      - name: "Advanced tests: BEAR & TIGER groups"
        continue-on-error: true
        run: |
          python -m pytest \
            tests/test_bear_dynamic_schema.py \
            tests/test_bear_historical_compat.py \
            tests/test_bear_migration.py \
            tests/test_bear_docker.py \
            tests/test_bear_refresh_e2e.py \
            tests/test_bear_validation_integration.py \
            tests/test_tiger_performance.py \
            tests/test_tiger_caching.py \
            tests/test_tiger_feedback.py \
            tests/test_tiger_validation.py \
            tests/test_reconciliation.py \
            -v --tb=short

      # ── Coverage report ─────────────────────────────────────────────────
      - name: Generate coverage report
        if: always()
        run: |
          python -m pytest tests/ \
            --ignore=tests/test_gui_tracker.py \
            --ignore=tests/optimization_validation \
            --cov=api --cov=utils \
            --cov-report=xml \
            --cov-report=term-missing \
            --cov-fail-under=80 \
            -q --tb=no 2>/dev/null || true

      - name: Upload coverage report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}
          path: coverage.xml
          retention-days: 7

      # ── Performance profiling ───────────────────────────────────────────
      - name: Run performance profiling
        run: python scripts/profile_queries.py --json
        continue-on-error: true

      - name: Upload profiling report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: profiling-${{ matrix.python-version }}
          path: profile_report.json
          retention-days: 7

      - name: Performance summary
        if: always()
        run: |
          if [ -f profile_report.json ]; then
            echo "## Performance Profiling Results" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "| Query | Time (ms) | Threshold (ms) | Status |" >> $GITHUB_STEP_SUMMARY
            echo "|-------|-----------|----------------|--------|" >> $GITHUB_STEP_SUMMARY
            python -c "
          import json, sys
          try:
              with open('profile_report.json') as f:
                  data = json.load(f)
              for key, ms in data.get('timings', {}).items():
                  threshold = data.get('thresholds', {}).get(key, 500)
                  status = 'PASS' if ms <= threshold else 'FAIL'
                  print(f'| {key} | {ms:.0f} | {threshold} | {status} |')
          except Exception as e:
              print(f'Error reading profile: {e}', file=sys.stderr)
          " >> $GITHUB_STEP_SUMMARY
          fi

  # Docker build validation
  docker-build:
    name: "Docker Build Validation"
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Build Docker image
        run: docker build -t dod-budget-test .

      - name: Validate Docker image imports
        run: |
          docker run --rm dod-budget-test python -c "from api.app import create_app; print('Import OK')"
